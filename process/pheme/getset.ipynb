{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropus: 101228\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from utils import *\n",
    "from copy import copy\n",
    "\n",
    "# dataSetPath = '../../../dataset/PHEME/'\n",
    "# dim = 5000\n",
    "\n",
    "'''\n",
    "threadIds 事件ID集合\n",
    "postIds 帖子ID集合\n",
    "rumorTags 事件对应的标签\n",
    "structures 事件包含的帖子及相应的回帖结构\n",
    "posts 每一个帖子的发布时间和内容\n",
    "users 每一个帖子对应的时间和用户信息\n",
    "label2Index 标签与谣言类型对应关系\n",
    "\"label2Index\": {\n",
    "\t\t\"non-rumor\": 0,\n",
    "\t\t\"true\": 1,\n",
    "\t\t\"false\": 2,\n",
    "\t\t\"unverified\": 3\n",
    "\t}\n",
    "'''\n",
    "\n",
    "with open('./trainSet.json', 'r') as f:\n",
    "    content = f.read()\n",
    "trainSet = json.loads(content)\n",
    "#帖子内容集合(按照事件及其事件顺序排序)\n",
    "threads = [] #[[事件所包含的帖子文本集合（按照时间排序）],...]\n",
    "threadUsers = []\n",
    "# print(len(trainSet['threadIds']))\n",
    "for threadId in trainSet['threadIds']:\n",
    "    thread = []\n",
    "    threadUser = []\n",
    "    #事件ID\n",
    "    structure = trainSet['structures'][threadId]\n",
    "    #事件中所包含的帖子的ID集合\n",
    "    ids = flattenStructure(structure)\n",
    "    #发帖时间--->帖子ID\n",
    "    time2Id = {}\n",
    "    time2IdUsers = {}\n",
    "    for id in ids:\n",
    "        if id in trainSet['posts']:\n",
    "            time2Id[str(trainSet['posts'][id]['time'])] = id\n",
    "            time2IdUsers[str(trainSet['users'][id]['created_at'])] = id\n",
    "    # post按照时间先后排序\n",
    "    time2Id = sorted(time2Id.items(), key=lambda d: d[0])\n",
    "    time2IdUsers = sorted(time2IdUsers.items(), key=lambda d: d[0])\n",
    "    for (time, id) in time2Id:\n",
    "        if id in trainSet['posts']:\n",
    "            #帖子的内容\n",
    "            thread.append(trainSet['posts'][id]['text'])\n",
    "    for (time, id) in time2IdUsers:\n",
    "        if id in trainSet['users']:\n",
    "            #帖子的内容\n",
    "            threadUser.append(trainSet['users'][id]['user'])\n",
    "    threads.append(thread)\n",
    "    threadUsers.append(threadUser)\n",
    "\n",
    "with open('./testSet.json', 'r') as f:\n",
    "    content = f.read()\n",
    "testSet = json.loads(content)\n",
    "# print(len(testSet['threadIds']))\n",
    "for threadId in testSet['threadIds']:\n",
    "    thread = []\n",
    "    threadUser = []\n",
    "    structure = testSet['structures'][threadId]\n",
    "    ids = flattenStructure(structure)\n",
    "    time2Id = {}\n",
    "    time2IdUsers = {}\n",
    "    for id in ids:\n",
    "        if id in testSet['posts']:\n",
    "            time2Id[str(testSet['posts'][id]['time'])] = id\n",
    "            time2IdUsers[str(testSet['users'][id]['created_at'])] = id\n",
    "    # post按照时间先后排序\n",
    "    time2Id = sorted(time2Id.items(), key=lambda d: d[0])\n",
    "    time2IdUsers = sorted(time2IdUsers.items(), key=lambda d: d[0])\n",
    "    for (time, id) in time2Id:\n",
    "        if id in testSet['posts']:\n",
    "            thread.append(testSet['posts'][id]['text'])\n",
    "    for (time, id) in time2IdUsers:\n",
    "        if id in trainSet['users']:\n",
    "            #帖子的内容\n",
    "            threadUser.append(testSet['users'][id]['user'])\n",
    "    threads.append(thread)\n",
    "    threadUsers.append(threadUser)\n",
    "\n",
    "cropus = []\n",
    "for thread in threads:\n",
    "    # print(f\"thread,{thread}\")\n",
    "    for text in thread:\n",
    "        # print(f\"text:{text}\")\n",
    "        cropus.append(text)\n",
    "# 存储的事每一个回复的文本[[],[],[]...]长度101228 代表所有的回复的数量有101228个（包括原始推文）\n",
    "print(\"cropus:\",len(cropus))\n",
    "\n",
    "# https://huaweicloud.csdn.net/638069c2dacf622b8df87387.html?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Eactivity-1-113820065-blog-123371873.235%5Ev30%5Epc_relevant_default_base3&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Eactivity-1-113820065-blog-123371873.235%5Ev30%5Epc_relevant_default_base3&utm_relevant_index=2\n",
    "#https://zhuanlan.zhihu.com/p/67883024\n",
    "# tfidf_vec = TfidfVectorizer(max_features=5000)#字典大小是5000 即如果进行向量嵌入则，每一个回复的文本都是一个5000维的tf-idf向量\n",
    "# tfidf_vec.fit(cropus)#只是建立了词典  没有对原始文本进行向量嵌入\n",
    "# print(f\"vocal:{len(tfidf_vec.vocabulary_)}\")\n",
    "label2IndexRumor = trainSet['label2Index'] if 'label2Index' in trainSet else None\n",
    "# print(len(tfidf_vec.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['threadIds', 'postIds', 'rumorTags', 'structures', 'posts', 'users', 'label2Index'])\n"
     ]
    }
   ],
   "source": [
    "with open('./testSet.json', 'r') as f:\n",
    "    content = f.read()\n",
    "testSet = json.loads(content)\n",
    "totalSet = {}\n",
    "for key in trainSet:\n",
    "    if isinstance(trainSet[key], list):\n",
    "        totalSet[key] = copy(trainSet[key])\n",
    "        totalSet[key] += testSet[key]\n",
    "    elif isinstance(trainSet[key], dict):\n",
    "        totalSet[key] = copy(trainSet[key])\n",
    "        totalSet[key].update(testSet[key])\n",
    "print(totalSet.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open('./PHEME_id_label.txt', 'w') as f:\n",
    "    for key in totalSet['rumorTags']:\n",
    "        f.write('{:s} {:d}\\n'.format(key, totalSet['rumorTags'][key]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def extract_usr_attributes(user_dict, source_post_time,reply_post_time):\n",
    "    # 拿到用户字典\n",
    "    # user_dict = post_dict['user']\n",
    "    user_dict = user_dict\n",
    "    # 本篇回复的创建时间\n",
    "    # reply_post_time = time_stamp(post_dict['created_at'])\n",
    "    # Extract User Attributes from user dictionary.\n",
    "    rep = []\n",
    "    vec = \"\"\n",
    "    # 1.是否使用\n",
    "    vec += '{:d} '.format(1 if user_dict['profile_use_background_image'] else 0)\n",
    "    # rep.append(1 if user_dict['profile_use_background_image'] else 0)\n",
    "    # 2.是否是认证账号\n",
    "    vec += '{:d} '.format(1 if user_dict['verified'] else 0)\n",
    "    # rep.append(1 if user_dict['verified'] else 0)\n",
    "    # 3.粉丝数\n",
    "    vec += '{:d} '.format(user_dict['followers_count'])\n",
    "    # rep.append(user_dict['followers_count'])\n",
    "    # 4.关注数\n",
    "    vec += '{:d} '.format(user_dict['listed_count'])\n",
    "    # rep.append(user_dict['listed_count'])\n",
    "    # 5.\n",
    "    vec += '{:d} '.format(user_dict['statuses_count'])\n",
    "    # rep.append(user_dict['statuses_count'])\n",
    "    # 6. Holy some users have no descriptions (NoneType) 自我描述的长度\n",
    "    if user_dict['description'] is not None:\n",
    "        #todo 这些地方是直接计算len  还是按照单词个数计算len比较好？\n",
    "        vec += '{:d} '.format(len(user_dict['description']))\n",
    "        # rep.append(len(user_dict['description']))\n",
    "    else:\n",
    "        vec += '{:d} '.format(0)\n",
    "        # rep.append(0)\n",
    "    # 7.\n",
    "    vec += '{:d} '.format(user_dict['friends_count'])\n",
    "    # rep.append(user_dict['friends_count'])\n",
    "    # 8.定位是否开启\n",
    "    vec += '{:d} '.format(1 if user_dict['geo_enabled'] else 0)\n",
    "    # rep.append(1 if user_dict['geo_enabled'] else 0)\n",
    "    # 9.\n",
    "    vec += '{:d} '.format(1 if user_dict['profile_background_tile'] else 0)\n",
    "    # rep.append(1 if user_dict['profile_background_tile'] else 0)\n",
    "    # 10.\n",
    "    vec += '{:d} '.format(user_dict['favourites_count'])\n",
    "    # rep.append(user_dict['favourites_count'])\n",
    "    # 11.\n",
    "    vec += '{:d} '.format(1 if user_dict['contributors_enabled'] else 0)\n",
    "    # rep.append(1 if user_dict['contributors_enabled'] else 0)\n",
    "    # 12. Reply time over the source post\n",
    "    vec += '{:f}\\t'.format(reply_post_time - source_post_time)\n",
    "    # rep.append(reply_post_time - source_post_time)\n",
    "    return vec\n",
    "\n",
    "def structure2graph(glove, structure,source_post_time, posts,users, contents, id, curIndex: list, parent=None):\n",
    "    if id in posts:\n",
    "        post = posts[id]['text']\n",
    "        user_dict = users[id]['user']\n",
    "        time = users[id]['created_at']\n",
    "        word2feq = {}\n",
    "        # for word in post.split(' '):\n",
    "        #     if word in word2feq:\n",
    "        #         word2feq[word] += 1\n",
    "        #     else:\n",
    "        #         word2feq[word] = 1\n",
    "\n",
    "\n",
    "\n",
    "        content = ''\n",
    "        if parent == None:\n",
    "            #id     None    curIndex[0]\n",
    "            content += '{:s}\\t{:s}\\t{:d}\\t'.format(id, 'None', curIndex[0])\n",
    "        else:\n",
    "            #id     parent    curIndex[0]\n",
    "            content += '{:s}\\t{:d}\\t{:d}\\t'.format(id, parent, curIndex[0])\n",
    "\n",
    "        user_vec = extract_usr_attributes(user_dict,source_post_time,time)\n",
    "        content += user_vec\n",
    "        vec = ''\n",
    "        #glove 词表中的index\n",
    "        for word in post.split(' '):\n",
    "            if word.lower() in glove.stoi:\n",
    "                vec += '{:d} '.format(glove.stoi[word.lower()])\n",
    "        # for word in word2feq:\n",
    "        #     if word in wordlist :\n",
    "        #         vec += '{:d}:{:d} '.format(wordlist[word], word2feq[word])\n",
    "        vec = vec[0:-1] + '\\n'\n",
    "        content += vec\n",
    "        if len(vec) == 1:\n",
    "            print(post)\n",
    "        else:\n",
    "            contents.append(content)\n",
    "\n",
    "    if not structure:\n",
    "        return\n",
    "\n",
    "    p = cur[0]\n",
    "    for cid in structure:\n",
    "        cur[0] += 1\n",
    "        structure2graph(glove, structure[cid],source_post_time, posts,users, contents, cid, curIndex, p)\n",
    "\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from config import glove\n",
    "with open('./PHEMEtree.txt', 'w') as f:\n",
    "    for tid in totalSet['threadIds']:\n",
    "        struct = totalSet['structures'][tid]\n",
    "        posts = totalSet['posts']\n",
    "        source_post_time = posts[tid]['time']\n",
    "        users = totalSet['users']\n",
    "        contents = []\n",
    "        cur = [1]\n",
    "        structure2graph(glove, struct, source_post_time,posts,users, contents, tid, cur, parent=None)\n",
    "        for content in contents:\n",
    "            f.write(content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
